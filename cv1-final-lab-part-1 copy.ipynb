{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <img src=\"https://miro.medium.com/v2/resize:fit:1250/format:webp/1*QgI1t-7yJApi4vQigFgsLQ.jpeg\" width=25% > </center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center> \n",
    "    <font size=\"6\">Final Lab (Part 1): Keypoint Detection, Bag of Visual Words and Image Classification</font>\n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Computer Vision 1 University of Amsterdam</font> \n",
    "</center>\n",
    "<center> \n",
    "    <font size=\"4\">Due 23:59PM, 24th October, 2025 (Amsterdam time)</font> \n",
    "</center>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "***\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<center>\n",
    "\n",
    "Student1 ID:  \\\n",
    "Student1 Name: \n",
    "\n",
    "Student2 ID: \\\n",
    "Student2 Name: \n",
    "\n",
    "Student3 ID: \\\n",
    "Student3 Name: \n",
    "\n",
    "( Student4 ID: \\\n",
    "Student4 Name: )\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Guidelines**\n",
    "\n",
    "Your code must be handed in this Jupyter notebook, renamed to **StudentID1_StudentID2_StudentID3.ipynb** before the deadline by submitting it to the Canvas Final Lab (Part 1) Assignment. Please also fill out your names and IDs above.\n",
    "\n",
    "For full credit, make sure your notebook follows these guidelines:\n",
    "\n",
    "- Please express your thoughts **concisely**. The number of words does not necessarily correlate with how well you understand the concepts.\n",
    "- Understand the problem as much as you can. When answering a question, provide evidence (qualitative and/or quantitative results, references to papers, figures, etc.) to support your arguments. Not everything might be explicitly asked for, so think about what might strengthen your arguments to make the notebook self-contained and complete.\n",
    "- Tables and figures must be accompanied by a **brief** description. Add a number, a title, and, if applicable, the name and unit of variables in a table, and name and unit of axes and legends in a figure.\n",
    "\n",
    "**Late submissions are not allowed.** Assignments submitted after the strict deadline will not be graded. In case of submission conflicts, TAs’ system clock is taken as reference. We strongly recommend submitting well in advance to avoid last-minute system failure issues.\n",
    "\n",
    "**Environment:** Since this is a project-based assignment, you are free to use any feature descriptor and machine learning tools (e.g., K-means, SVM). You should use Python for your implementation. You are free to use any Python library for this assignment, but make sure to provide a conda environment file!\n",
    "\n",
    "**Plagiarism Note:** Keep in mind that plagiarism (submitted materials which are not your work) is a serious offense and any misconduct will be addressed according to university regulations. This includes using generative tools such as ChatGPT.\n",
    "\n",
    "**Ensure that you save all results/answers to the questions (even if you reuse some code).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Report Preparation**\n",
    "\n",
    "Your tasks include the following:\n",
    "\n",
    "1. **Report Preparation:** For both parts of the final project, students are expected to prepare a report. The report should include all details on implementation approaches, analysis of results for different settings, and visualizations illustrating experiments and performance of your implementation. Grading will be based on the report, so it should be as self-contained as possible. If the report contains faulty results or ambiguities, TAs can refer to your code for clarification. Only section 10 of this notebook should **not** be included in the report.\n",
    "\n",
    "2. **Explanation of Results:** Do not just provide numbers without explanation. Discuss different settings to show your understanding of the material and processes involved.\n",
    "\n",
    "3. **Quantitative Evaluation:** For quantitative evaluation, you are expected to provide the results based on the mAP (mean Average Precision) metric. You should report the mAP for each experimental setup. \n",
    "\n",
    "4. **Qualitative Evaluation:** For qualitative evaluation, you are expected to visualize the top-5 and bottom-5 ranked test images (based on classifier confidence for the target class) per setup. Provide a figure for each experimental setup Visual elements such as charts, graphs, and plots are always useful. Keep this in mind while writing your reports.\n",
    "\n",
    "5. **Aim:** Understand the basic Image Classification pipeline using a traditional Bag of Visual Words method.\n",
    "\n",
    "6. **Working on Assignments:** Students should work in assigned groups for **two** weeks. Any questions can be discussed on ED.\n",
    "\n",
    "    - **Submission:** Submit your source code and report together in a zip file (`ID1_ID2_ID3_part1.zip`). The report should be a maximum of 10 pages (single-column, including tables and figures, excluding references and appendix). Express thoughts concisely. Tables and figures must be accompanied by a description. Number them and, if applicable, name variables in tables, and label axes in figures.\n",
    "\n",
    "7. **Hyperparameter Search:** In your experiments, remember to perform a hyperparameter search to find the optimal settings for your classifier. Clearly document the search process, the parameters you explored, and how they influenced the performance of your model.\n",
    "\n",
    "8. **Format and Testing:** The report should be in **PDF format**, and the code in **.ipynb format**. Test that all functionality works as expected in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**\n",
    "\n",
    "- [Section 1: Data Preparation (0 points)](#section-1)\n",
    "- [Section 2: Keypoint Detection and Feature Extraction (3 points)](#section-2)\n",
    "- [Section 3: Building the Visual Vocabulary (3 points)](#section-3)\n",
    "- [Section 4: Encoding Train Image Features (3 points)](#section-4)\n",
    "- [Section 5: Visualizing the Bag of Visual Words for Each Class (3 points)](#section-5)\n",
    "- [Section 6: Encoding Test Image Features (0 points)](#section-6)\n",
    "- [Section 7: Training the Classifiers (5 points)](#section-7)\n",
    "- [Section 8: Evaluating the Classifiers (12 points)](#section-8)\n",
    "- [Section 9: Hyperparameter Search (16 points)](#section-9)\n",
    "- [Section 10: Using CLIP for Image Classification (5 points)](#section-10)\n",
    "- [Section X: Individual Contribution Report (Mandatory)](#section-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Section 1: Data Preparation (0 points)**\n",
    "\n",
    "The goal of this lab is to implement an image classification system that can identify objects from a given set of classes. You will perform a 5-class image classification using a bag-of-words approach ([reference](http://www.robots.ox.ac.uk/~az/icvss08_az_bow.pdf)). The classes for this task are:\n",
    "\n",
    "1. **Frog**\n",
    "2. **Automobile**\n",
    "3. **Bird**\n",
    "4. **Cat**\n",
    "5. **Deer**\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) will be used for this task. This dataset contains 32x32 pixel RGB images, divided into sub-directories with 5000 training images and 1000 test images for each class.\n",
    "\n",
    "The dataset will be automatically downloaded using the code provided in this notebook. You will need to perform training on the training set, which will later be divided into two subsets: one for building the visual vocabulary and another for training the classifier. Using more samples for training generally results in better performance. However, if computational resources are limited, you may use fewer training images to save time, as long as at least 500 images per class are included.\n",
    "\n",
    "The system must be tested using the specified subset of test images. Use all 1000 test images (per class) to observe the full performance of the model. Ensure that test images are excluded from training to maintain a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Define total train and test sizes\n",
    "total_train_size = 5000  # Default value for total training images\n",
    "total_test_size = 1000   # Default value for total test images\n",
    "\n",
    "# Define batch sizes for DataLoader\n",
    "train_batch_size = total_train_size\n",
    "test_batch_size = total_test_size\n",
    "\n",
    "# Define the number of Visual Words\n",
    "num_of_visual_words = 1000  # Default value for number of visual words\n",
    "\n",
    "# Number of classes\n",
    "num_classes = 5\n",
    "\n",
    "# Compute images per class for training and testing\n",
    "images_per_class_train = total_train_size // num_classes  # e.g., 5000 // 5 = 1000 per class\n",
    "images_per_class_test = total_test_size // num_classes    # e.g., 1000 // 5 = 200 per class\n",
    "\n",
    "# Define the transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  \n",
    "])\n",
    "\n",
    "# Define the class indices for the 5 selected classes: frog, automobile, bird, cat, and deer\n",
    "selected_classes = [6, 1, 2, 3, 4]  # 6: frog, 1: automobile, 2: bird, 3: cat, 4: deer\n",
    "class_to_label = {orig_class: new_label for new_label, orig_class in enumerate(selected_classes)}\n",
    "\n",
    "# Load the CIFAR-10 training set\n",
    "train_set = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Function to filter and remap dataset\n",
    "def filter_dataset(dataset, images_per_class, selected_classes, class_to_label):\n",
    "    selected_indices = []\n",
    "    class_counts = {class_idx: 0 for class_idx in selected_classes}\n",
    "    remapped_labels = []\n",
    "\n",
    "    for idx, (image, label) in enumerate(dataset):\n",
    "        if label in selected_classes and class_counts[label] < images_per_class:\n",
    "            selected_indices.append(idx)\n",
    "            remapped_labels.append(class_to_label[label])\n",
    "            class_counts[label] += 1\n",
    "\n",
    "            # Stop if we have enough samples for each class\n",
    "            if all(count >= images_per_class for count in class_counts.values()):\n",
    "                break\n",
    "\n",
    "    filtered_dataset = Subset(dataset, selected_indices)\n",
    "    return filtered_dataset, remapped_labels\n",
    "\n",
    "# Filter and remap training set\n",
    "filtered_train_set, train_mapped_labels = filter_dataset(train_set, images_per_class_train, selected_classes, class_to_label)\n",
    "\n",
    "# Load the CIFAR-10 test set\n",
    "test_set = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Filter and remap test set\n",
    "filtered_test_set, test_mapped_labels = filter_dataset(test_set, images_per_class_test, selected_classes, class_to_label)\n",
    "\n",
    "# Create data loaders for the filtered datasets\n",
    "train_data_loader = DataLoader(filtered_train_set, batch_size=train_batch_size, shuffle=False)\n",
    "test_data_loader = DataLoader(filtered_test_set, batch_size=test_batch_size, shuffle=False)\n",
    "\n",
    "# Extract all training data and remapped labels\n",
    "train_images, _ = next(iter(train_data_loader))\n",
    "train_labels = torch.tensor(train_mapped_labels)\n",
    "\n",
    "train_images = train_images.permute(0, 2, 3, 1)\n",
    "print(f\"Filtered train data: {train_images.shape}\")\n",
    "print(f\"Filtered train labels: {train_labels.shape}\")\n",
    "\n",
    "# Extract all test data and remapped labels\n",
    "test_images, _ = next(iter(test_data_loader))\n",
    "test_labels = torch.tensor(test_mapped_labels)\n",
    "\n",
    "test_images = test_images.permute(0, 2, 3, 1)\n",
    "print(f\"Filtered test data: {test_images.shape}\")\n",
    "print(f\"Filtered test labels: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "### **Section 2: Keypoint Detection and Feature Extraction (3 points)**\n",
    "\n",
    "In this section, you will work on detecting keypoints and extracting features from the dataset. Your task is to use **two different feature extraction techniques** to identify keypoints in the images. Visualize two images from each of the five classes (Frog, Automobile, Bird, Cat, Deer) for both feature extraction techniques. For each image, draw circles around the detected keypoints that represent their size.\n",
    "\n",
    "This step is essential to understand how different feature extractors behave across various classes, setting the foundation for further analysis and classification in later steps.\n",
    "\n",
    "**Hint:** You can use the OpenCV library to detect keypoints and extract features. You can also upscale the images to improve the visualization of the keypoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize_image(img_tensor):\n",
    "    \"\"\"Convert normalized tensor image to uint8 numpy array\"\"\"\n",
    "    img = img_tensor.numpy()\n",
    "    img = (img * 0.5 + 0.5) * 255  # Denormalize from [-1, 1] to [0, 255]\n",
    "    img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "    return img\n",
    "\n",
    "\n",
    "def detect_keypoints_and_extract(image, detector):\n",
    "    \"\"\"\n",
    "    Detect keypoints and extract descriptors\n",
    "    \n",
    "    Args:\n",
    "        image: input image \n",
    "        detector: feature detector object \n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    keypoints, descriptors = detector.detectAndCompute(gray, None)\n",
    "    return keypoints, descriptors\n",
    "\n",
    "\n",
    "def visualize_keypoints(images, labels, class_names, detector_name, detector, num_images_per_class=2, upscale_factor=4):\n",
    "    \"\"\"\n",
    "    Visualize keypoints for selected images from each class\n",
    "    \n",
    "    Args:\n",
    "        images: tensor of images\n",
    "        labels: tensor of labels\n",
    "        class_names: list of class names\n",
    "        detector_name: name of the detector\n",
    "        detector: detector object\n",
    "        num_images_per_class: number of images to show per class\n",
    "        upscale_factor: factor to upscale images for better visualization\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(num_classes, num_images_per_class, figsize=(6*num_images_per_class, 6*num_classes))\n",
    "\n",
    "    if num_images_per_class == 1: axes = axes.reshape(-1, 1)\n",
    "\n",
    "    fig.suptitle(f'Keypoint Detection with {detector_name}', fontsize=16, y=0.995)\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        # Get indices for the first num_images_per_class images of current class\n",
    "        class_mask = labels == class_idx # <- mask\n",
    "        class_indices = torch.where(class_mask)[0]\n",
    "        selected_indices = class_indices[:num_images_per_class]\n",
    "        \n",
    "        for img_idx, idx in enumerate(selected_indices):\n",
    "            img_tensor = images[idx]\n",
    "            img = denormalize_image(img_tensor)\n",
    "            \n",
    "            # upscale\n",
    "            upscaled_img = cv2.resize(img, None, fx=upscale_factor, fy=upscale_factor, interpolation=cv2.INTER_CUBIC)\n",
    "            \n",
    "            # detect keypoints\n",
    "            keypoints, _ = detect_keypoints_and_extract(upscaled_img, detector)\n",
    "            \n",
    "            # draw keypoints \n",
    "            img_with_keypoints = cv2.drawKeypoints(\n",
    "                upscaled_img,\n",
    "                keypoints, \n",
    "                None,\n",
    "                flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS,\n",
    "                color=(0, 255, 0)\n",
    "            )\n",
    "            \n",
    "\n",
    "            ax = axes[class_idx, img_idx]\n",
    "            ax.imshow(img_with_keypoints)\n",
    "            ax.set_title(f'{class_names[class_idx]}\\n{len(keypoints)} keypoints',  fontsize=12)\n",
    "            ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_to_class_name = {\n",
    "    0: 'Frog',\n",
    "    1: 'Automobile',\n",
    "    2: 'Bird',\n",
    "    3: 'Cat',\n",
    "    4: 'Deer'\n",
    "}\n",
    "class_names = [label_to_class_name[i] for i in range(len(label_to_class_name))]\n",
    "\n",
    "\n",
    "# 1. SIFT\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SIFT\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "# Initialize detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# Visualize keypoints\n",
    "fig1 = visualize_keypoints(\n",
    "    train_images,\n",
    "    train_labels,\n",
    "    class_names,\n",
    "    'SIFT',\n",
    "    sift,\n",
    "    num_images_per_class=2,\n",
    "    upscale_factor=4\n",
    ")\n",
    "\n",
    "\n",
    "# 2. ORB\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ORB\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize detector\n",
    "orb = cv2.ORB_create(nfeatures=1000)  \n",
    "\n",
    "# Visualize ORB keypoints\n",
    "fig2 = visualize_keypoints(\n",
    "    train_images, \n",
    "    train_labels, \n",
    "    class_names,\n",
    "    'ORB',\n",
    "    orb,\n",
    "    num_images_per_class=2,\n",
    "    upscale_factor=4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_keypoint_stats(images, labels, class_names, detector, upscale_factor=4):\n",
    "    \"\"\"\n",
    "    Compute average number of keypoints per class\n",
    "    \"\"\"\n",
    "\n",
    "    stats = {name: [] for name in class_names}\n",
    "    \n",
    "    for idx in range(len(images)):\n",
    "        img = denormalize_image(images[idx])\n",
    "        scaled_img = cv2.resize(img, None, fx=upscale_factor, fy=upscale_factor, interpolation=cv2.INTER_CUBIC)\n",
    "        keypoints, _ = detect_keypoints_and_extract(scaled_img, detector)\n",
    "        class_name = class_names[labels[idx].item()]\n",
    "        stats[class_name].append(len(keypoints))\n",
    "    \n",
    "    for class_name in class_names:\n",
    "        avg_kp = np.mean(stats[class_name])\n",
    "        std_kp = np.std(stats[class_name])\n",
    "        print(f\"  {class_name:12s}: {avg_kp:6.2f} ± {std_kp:5.2f} keypoints\")\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics for both detectors\n",
    "sample_size = 100 \n",
    "\n",
    "sample_indices = []\n",
    "for class_idx in range(num_classes):\n",
    "    class_mask = train_labels == class_idx\n",
    "    class_indices = torch.where(class_mask)[0][:sample_size]\n",
    "    sample_indices.extend(class_indices.tolist())\n",
    "\n",
    "sample_images = train_images[sample_indices]\n",
    "sample_labels = train_labels[sample_indices]\n",
    "\n",
    "\n",
    "print(\"SIFT statistics:\")\n",
    "stats_sift = compute_keypoint_stats(sample_images, sample_labels, class_names, sift, 4)\n",
    "print(\"\\nORB statistics:\")\n",
    "stats_orb = compute_keypoint_stats(sample_images, sample_labels, class_names, orb, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3\"></a>\n",
    "### **Section 3: Building the Visual Vocabulary (3 points)**\n",
    "\n",
    "In this section, the task is to create a visual vocabulary by clustering feature descriptors extracted from the images using K-Means. Each cluster center in this vocabulary will represent a visual word. Use the two different extraction techniques you implemented to extract descriptors from a subset of training images that includes all categories, and then apply K-Means clustering to build the vocabulary. The number of clusters is fixed at 1000, but you can experiment with different values when you are tuning the hyperparameters in section 9.\n",
    "\n",
    "To examine the effect of different amounts of training data, build separate visual vocabularies using 30%, 40%, and 50% subsets of the training images. For faster clustering, the `faiss` library can be used, as it provides an efficient implementation of K-Means. Then, visualize the first 10 clusters for each feature extraction technique and each subset size using PCA to reduce the dimensions to 2D.\n",
    "\n",
    "**Hints:**\n",
    "1. Begin by debugging the code with a small number of input images to ensure it functions correctly before running it on larger datasets.\n",
    "2. If the `faiss` library is not available, K-Means clustering can also be performed using the `sklearn` or `scipy` libraries.\n",
    "3. For visualization, use PCA from `sklearn.decomposition` to reduce the high-dimensional descriptors to 2D. Display up to 10 clusters in the scatter plot to maintain clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans \n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data_percentages = [0.3, 0.4, 0.5]  # 30%, 40%, 50% of training data\n",
    "\n",
    "upscale_factor = 4 \n",
    "\n",
    "\n",
    "def build_visual_vocabulary(descriptors, n_clusters=1000):\n",
    "    \"\"\"\n",
    "    Build visual vocabulary via k-means clustering\n",
    "    \n",
    "    Args:\n",
    "        descriptors: numpy array of descriptors\n",
    "        n_clusters: number of clusters \n",
    "    \n",
    "    Returns:\n",
    "        kmeans: fitted K-Means model\n",
    "        cluster_centers: cluster centers (visual words)\n",
    "    \"\"\"\n",
    "\n",
    "    kmeans = MiniBatchKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        random_state=42,\n",
    "        max_iter=100,\n",
    "        verbose=0,\n",
    "        n_init=4,\n",
    "        batch_size=1000\n",
    "    )\n",
    "    \n",
    "    kmeans.fit(descriptors)\n",
    "    \n",
    "    return kmeans, kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "def extract_all_descriptors(images, detector, subset_percentage, upscale_factor=4):\n",
    "    \"\"\"\n",
    "    Extract descriptors from a subset of images\n",
    "    \n",
    "    Args:\n",
    "        images: tensor of images\n",
    "        detector: feature detector \n",
    "        subset_percentage: percentage training data to use \n",
    "        upscale_factor: factor to upscale images\n",
    "    \n",
    "    Returns:\n",
    "        all_descriptors: numpy array of all descriptors\n",
    "        num_images_used: number of images processed\n",
    "    \"\"\"\n",
    "    num_images = int(len(images) * subset_percentage)\n",
    "    all_descriptors = []\n",
    "    images_with_descriptors = 0\n",
    "    \n",
    "    print(f\"Extracting descriptors from {num_images} images...\")\n",
    "    \n",
    "    for idx in range(num_images):\n",
    "        # Denormalize image\n",
    "        img = denormalize_image(images[idx])\n",
    "        \n",
    "        # Detect keypoints and extract descriptors\n",
    "        upscaled_img = cv2.resize(img, None, fx=upscale_factor, fy=upscale_factor, interpolation=cv2.INTER_CUBIC)\n",
    "        _, descriptors = detect_keypoints_and_extract(upscaled_img, detector)\n",
    "        \n",
    "        if descriptors is not None and len(descriptors) > 0:\n",
    "            all_descriptors.append(descriptors)\n",
    "            images_with_descriptors += 1\n",
    "        \n",
    "\n",
    "    if len(all_descriptors) > 0: all_descriptors = np.vstack(all_descriptors)\n",
    "    else: all_descriptors = np.array([])\n",
    "    \n",
    "    print(f\"  Total descriptors extracted: {len(all_descriptors)}\")\n",
    "    print(f\"  Images with descriptors: {images_with_descriptors}/{num_images}\")\n",
    "    \n",
    "    return all_descriptors, num_images\n",
    "\n",
    "\n",
    "\n",
    "def visualize_clusters_pca(descriptors, labels, cluster_centers, detector_name, subset_percentage, n=10):\n",
    "    \"\"\"\n",
    "    Visualize clusters using PCA\n",
    "    \n",
    "    Args:\n",
    "        descriptors: descriptors\n",
    "        labels: cluster labels for each descriptor\n",
    "        cluster_centers: cluster centers\n",
    "        detector_name: name of the feature detector\n",
    "        subset_percentage: percentage of training data used\n",
    "        n: number of clusters to display (10)\n",
    "    \"\"\"\n",
    "    print(f\"visualizing first {n} clusters using PCA...\")\n",
    "\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    \n",
    "    # get only first n clusters\n",
    "    mask = labels < n\n",
    "    descriptors_subset = descriptors[mask]\n",
    "    labels_subset = labels[mask]\n",
    "\n",
    "    # transform descriptors and cluster centers to 2D\n",
    "    descriptors_pca = pca.fit_transform(descriptors_subset)\n",
    "    centers_pca = pca.transform(cluster_centers[:n])\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, n))\n",
    "    for cluster_id in range(n):\n",
    "        cluster_mask = labels_subset == cluster_id\n",
    "        plt.scatter(\n",
    "            descriptors_pca[cluster_mask, 0],\n",
    "            descriptors_pca[cluster_mask, 1],\n",
    "            c=[colors[cluster_id]],\n",
    "            label=f'Cluster {cluster_id}',\n",
    "            alpha=0.6,\n",
    "            s=20\n",
    "        )\n",
    "    \n",
    "    plt.scatter(\n",
    "        centers_pca[:, 0],\n",
    "        centers_pca[:, 1],\n",
    "        c='black',\n",
    "        marker='X',\n",
    "        s=200,\n",
    "        edgecolors='white',\n",
    "        linewidths=2,\n",
    "        label='Cluster Centers',\n",
    "        zorder=10\n",
    "    )\n",
    "\n",
    "    plt.title(f'Visual Vocabulary ({int(subset_percentage*100)}% training data, {detector_name})\\n')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.legend(loc='best', fontsize=9, ncol=2)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularies = {\n",
    "    'SIFT': {},\n",
    "    'ORB': {}\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for detector_name, detector in [('SIFT', sift), ('ORB', orb)]:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Building vocabulary for {detector_name}...\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for percentage in data_percentages:\n",
    "        print(f\"Processing for {int(percentage*100)}% of training data\")\n",
    "        \n",
    "        # extract descriptors\n",
    "        voc_descriptors, voc_num_images = extract_all_descriptors(\n",
    "            train_images, detector, percentage, upscale_factor\n",
    "        )\n",
    "        \n",
    "        if len(voc_descriptors) == 0:\n",
    "            print(f\"No descriptors extracted for {detector}\")\n",
    "            continue\n",
    "\n",
    "        voc_descriptors = voc_descriptors.astype(np.float32)\n",
    "        \n",
    "        # build vocabulary\n",
    "        kmeans, cluster_centers = build_visual_vocabulary(\n",
    "            voc_descriptors, \n",
    "            n_clusters=num_of_visual_words\n",
    "        )\n",
    "        \n",
    "        # Store vocabulary\n",
    "        vocabularies[detector_name][percentage] = {\n",
    "            'kmeans': kmeans,\n",
    "            'cluster_centers': cluster_centers,\n",
    "            'num_descriptors': len(voc_descriptors),\n",
    "            'num_images': voc_num_images\n",
    "        }\n",
    "        \n",
    "        # predict cluster labels for visualization\n",
    "        labels = kmeans.predict(voc_descriptors)\n",
    "        \n",
    "        # Visualize clusters\n",
    "        visualize_clusters_pca(\n",
    "            voc_descriptors,\n",
    "            labels,\n",
    "            cluster_centers,\n",
    "            detector_name,\n",
    "            percentage,\n",
    "            n=10\n",
    "        )\n",
    "\n",
    "# Summary of vocabularies built\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for detector in ['SIFT', 'ORB']:\n",
    "    print(f\"\\n{detector}:\")\n",
    "    for percentage in data_percentages:\n",
    "        if percentage in vocabularies[detector]:\n",
    "            vocab = vocabularies[detector][percentage]\n",
    "            print(f\"  {int(percentage*100)}% data: \"\n",
    "                  f\"{vocab['num_descriptors']:,} descriptors from \"\n",
    "                  f\"{vocab['num_images']} images → \"\n",
    "                  f\"{num_of_visual_words} visual words\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-4\"></a>\n",
    "### **Section 4: Encoding Train Image Features (3 points)**\n",
    "\n",
    "In this section, the task is to encode image features using the visual vocabulary created earlier. Each image will be represented as a histogram of visual words, reflecting the frequency of each visual word in the image. This representation will allow for comparing images based on their visual content.\n",
    "\n",
    "To encode an image, identify the nearest visual word (cluster center) for each feature descriptor extracted from the image. Construct a histogram that counts the occurrences of each visual word within the image. The final output will be a collection of histograms, one for each image, where each histogram serves as the feature representation of that image. Once again,  Use the two different extraction techniques you implemented to extract descriptors from the images. Then, encode the images using the visual vocabulary created in the previous step.\n",
    "\n",
    "**Hint:** Utilize the `faiss` library for efficient nearest neighbor search when assigning each descriptor to the nearest cluster center in the visual dictionary. If `faiss` is not available, consider using other libraries, such as `scikit-learn`, for this step. Once the histograms are obtained, they will be used for further tasks, such as training a classifier. For now, perform the encoding only for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image_bovw(image, detector, vocabulary, upscale_factor=4):\n",
    "    \"\"\"\n",
    "    Encode a single image as a histogram of visual words \n",
    "    \n",
    "    Args:\n",
    "        image: normalized tensor image\n",
    "        detector: feature detector\n",
    "        vocabulary: k-means dictionary\n",
    "        upscale_factor: upscale factor\n",
    "    \n",
    "    Returns:\n",
    "        histogram: BoVW histogram\n",
    "    \"\"\"\n",
    "    \n",
    "    img = denormalize_image(image)\n",
    "    \n",
    "    # 1) extract descriptors\n",
    "    img = cv2.resize(img, None, fx=upscale_factor, fy=upscale_factor, interpolation=cv2.INTER_CUBIC)      \n",
    "    _, descriptors = detect_keypoints_and_extract(img, detector)\n",
    "\n",
    "    # 2) init histogram\n",
    "    num_words = len(vocabulary['cluster_centers'])\n",
    "    histogram = np.zeros(num_words, dtype=np.float32)\n",
    "    \n",
    "    if descriptors is None or len(descriptors) == 0: return histogram\n",
    "\n",
    "    descriptors = descriptors.astype(np.float32)\n",
    "\n",
    "    # 3) predict visual words of the image \n",
    "    kmeans = vocabulary['kmeans']\n",
    "    labels = kmeans.predict(descriptors)\n",
    "\n",
    "    # 4) create histogram\n",
    "    for label in labels: histogram[label] += 1\n",
    "    \n",
    "    # 5) normalization\n",
    "    if histogram.sum() > 0: histogram = histogram / histogram.sum()\n",
    "    \n",
    "    return histogram\n",
    "\n",
    "\n",
    "\n",
    "def encode_dataset_bovw(images, labels, detector, vocabulary, detector_name, upscale_factor=4):\n",
    "    \"\"\"\n",
    "    Encode all images in dataset as BoVW histograms\n",
    "    \n",
    "    Args:\n",
    "        images: tensor of images\n",
    "        labels: tensor of labels\n",
    "        detector: feature detector\n",
    "        vocabulary: vocabulary dictionary\n",
    "        detector_name: name of detector (for printing)\n",
    "        upscale_factor: factor to upscale images\n",
    "\n",
    "    Returns:\n",
    "        bovw_features: numpy array of shape (num_images, num_visual_words)\n",
    "        valid_indices: indices of images with features (some might have no keypoints)\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "\n",
    "    print(f\"Encoding {num_images} images using {detector_name}...\")\n",
    "\n",
    "    bovw_features = []\n",
    "    valid_indices = []\n",
    "\n",
    "    for idx in range(num_images):\n",
    "        # Encode image\n",
    "        hist = encode_image_bovw(images[idx], detector, vocabulary, upscale_factor)\n",
    "        if hist.sum() > 0:\n",
    "            bovw_features.append(hist)\n",
    "            valid_indices.append(idx)\n",
    "\n",
    "    \n",
    "    bovw_features = np.array(bovw_features, dtype=np.float32)\n",
    "    valid_indices = np.array(valid_indices)\n",
    "\n",
    "    print(f\"Valid images: {len(valid_indices)}/{num_images}\")\n",
    "    print(f\"Feature shape: {bovw_features.shape}\")\n",
    "\n",
    "    return bovw_features, valid_indices\n",
    "    \n",
    "  \n",
    "\n",
    "def visualize_bovw_histograms(bovw_features, labels, class_names, detector_name, num_samples=3, n=50):\n",
    "    \"\"\"\n",
    "    Visualize BoVW histograms \n",
    "    \n",
    "    Args:\n",
    "        bovw_features: BoVW feature vectors (one histogram for each img)\n",
    "        labels: image labels \n",
    "        class_names: list of class names\n",
    "        detector_name: name of detector\n",
    "        num_samples: number of samples per class to show\n",
    "        num_words_to_show: number of visual words to display in histogram\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_classes, num_samples, figsize=(15, 10))\n",
    "    fig.suptitle(f'BoVW - {detector_name}')\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        class_mask = labels == class_idx\n",
    "        class_indices = np.where(class_mask)[0]\n",
    "        \n",
    "        # get samples\n",
    "        if len(class_indices) >= num_samples: selected = np.random.choice(class_indices, num_samples, replace=False)\n",
    "        else: selected = class_indices[:num_samples]\n",
    "        \n",
    "        for sample_idx, img_idx in enumerate(selected):\n",
    "            ax = axes[class_idx, sample_idx]\n",
    "            hist = bovw_features[img_idx]\n",
    "     \n",
    "            ax.bar(range(n), hist[:n], color='steelblue', alpha=0.7)\n",
    "            \n",
    "    \n",
    "            if sample_idx == 0: ax.set_ylabel(class_names[class_idx], fontsize=11, fontweight='bold')\n",
    "            if class_idx == 0: ax.set_title(f'Sample {sample_idx + 1}', fontsize=10)\n",
    "            if class_idx == num_classes - 1: ax.set_xlabel('Visual Word', fontsize=9)\n",
    "            \n",
    "            ax.set_ylim([0, hist[:n].max() * 1.2 if hist[:n].max() > 0 else 0.1])\n",
    "            ax.tick_params(labelsize=8)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_features = {\n",
    "    'SIFT': {},\n",
    "    'ORB': {}\n",
    "}\n",
    "\n",
    "\n",
    "for detector_name, detector in [('SIFT', sift), ('ORB', orb)]:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Encoding training images - {detector_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for percentage in data_percentages:\n",
    "        print(f\"\\n{'*'*70}\")\n",
    "        print(f\"Processing {int(percentage*100)}% vocabulary\")\n",
    "        print(f\"\\n{'*'*70}\")\n",
    "        \n",
    "        if percentage not in vocabularies[detector_name]: continue\n",
    "        \n",
    "        # get vocabulary\n",
    "        vocabulary = vocabularies[detector_name][percentage]\n",
    "\n",
    "        # encode all training images \n",
    "        bovw, valid_indices = encode_dataset_bovw(\n",
    "            train_images,\n",
    "            train_labels,\n",
    "            detector,\n",
    "            vocabulary,\n",
    "            detector_name,\n",
    "            upscale_factor\n",
    "        )\n",
    "\n",
    "        encoded_train_features[detector_name][percentage] = {\n",
    "            'features': bovw,\n",
    "            'labels': train_labels[valid_indices],\n",
    "            'valid_indices': valid_indices\n",
    "        }\n",
    "\n",
    "       \n",
    "        # visualize histograms\n",
    "        visualize_bovw_histograms(\n",
    "            bovw,\n",
    "            train_labels[valid_indices],\n",
    "            [label_to_class_name[i] for i in range(len(label_to_class_name))],\n",
    "            f\"{detector_name} ({int(percentage*100)}% vocab)\",\n",
    "            num_samples=3,\n",
    "            n=200\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sparsity analysis\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Sparsity Analysis of BoVW Histograms\")\n",
    "for detector_name in ['SIFT', 'ORB']:\n",
    "    for percentage in data_percentages:\n",
    "        if percentage in encoded_train_features[detector_name]:\n",
    "            features = encoded_train_features[detector_name][percentage]['features']\n",
    "            labels = encoded_train_features[detector_name][percentage]['labels']\n",
    "\n",
    "            non_empty_count = (features.sum(axis=1) > 0).sum()\n",
    "            total_count = len(features)\n",
    "            \n",
    "            print(f\"\\n{detector_name} {int(percentage*100)}%:\")\n",
    "            print(f\"Non-empty histograms: {non_empty_count}/{total_count} ({non_empty_count/total_count*100:.1f}%)\")\n",
    "\n",
    "\n",
    "            for class_idx in range(num_classes):\n",
    "                class_mask = labels == class_idx\n",
    "                class_features = features[class_mask]\n",
    "                non_empty_in_class = (class_features.sum(axis=1) > 0).sum()\n",
    "                min_number_of_visual_words_for_class = np.min((class_features > 0).sum(axis=1)) \n",
    "\n",
    "                print(f\"{class_names[class_idx]}: {non_empty_in_class}/{class_mask.sum()} non-empty, min number of visual words: {min_number_of_visual_words_for_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-5\"></a>\n",
    "### **Section 5: Visualizing the Bag of Visual Words for Each Class (3 points)**\n",
    "\n",
    "In this section, the task is to visualize the Bag of Visual Words for each class using the histograms generated in the previous step. The goal is to plot the mean histogram of visual words for each class, showing the distribution of visual words across the different categories in the training set.\n",
    "\n",
    "Use the two different extraction techniques you implemented for this visualization. For each technique, calculate the mean histogram for each class and create a bar plot to display these histograms. Ensure that the plots are labeled clearly with the class names and feature descriptor types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_mean_bovw_per_class(features, labels, class_names, detector_name, percentage):\n",
    "    \"\"\"\n",
    "    Visualize mean BoVW histogram for each class\n",
    "    \n",
    "    Args:\n",
    "        features: feature vectors (num_images, num_visual_words)\n",
    "        labels: image labels\n",
    "        class_names: list of class names\n",
    "        detector_name: name of detector\n",
    "        percentage: percentage of training data used\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes = len(class_names)\n",
    "    num_visual_words = features.shape[1]\n",
    "    \n",
    "\n",
    "    mean_histograms = [] # one per class\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        class_mask = labels == class_idx\n",
    "        mean_histograms.append(features[class_mask].mean(axis=0))\n",
    "    \n",
    "    mean_histograms = np.array(mean_histograms) # (num_classes, num_visual_words)\n",
    "    \n",
    "    \n",
    "    fig, axes = plt.subplots(num_classes, 1, figsize=(15, 3*num_classes))\n",
    "    if num_classes == 1: axes = [axes]\n",
    "\n",
    "    fig.suptitle(f'Mean BoVW per Class - {detector_name} ({int(percentage*100)}%)', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        ax = axes[class_idx]\n",
    "    \n",
    "        ax.bar(range(num_visual_words), mean_histograms[class_idx], color=\"green\", alpha=0.4, width=1.1)\n",
    "        \n",
    "        ax.set_ylabel('Mean Frequency')\n",
    "        ax.set_title(f'{class_names[class_idx]}', fontweight='bold', loc='left')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        ax.set_xlabel('Visual Word')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "for detector_name in ['SIFT', 'ORB']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"{detector_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    for percentage in data_percentages:\n",
    "        if percentage in encoded_train_features[detector_name]:\n",
    "            visualize_mean_bovw_per_class(\n",
    "                features = encoded_train_features[detector_name][percentage]['features'],\n",
    "                labels = encoded_train_features[detector_name][percentage]['labels'],\n",
    "                class_names = class_names,\n",
    "                detector_name = detector_name,\n",
    "                percentage = percentage\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-6\"></a>\n",
    "### **Section 6: Encoding Test Image Features (0 points)**\n",
    "\n",
    "In this section, the task is to encode the test image features using the visual vocabulary created from the training set. Similar to the previous encoding step, each test image will be represented as a histogram of visual words, which will then be used for evaluating classification performance.\n",
    "\n",
    "Use the same two feature extraction techniques you selected earlier. Extract keypoints and descriptors for the test images, then encode these images using the visual vocabulary. This will allow you to compare the encoded features of test images against those of the training set.\n",
    "\n",
    "**Hint:** Reuse the functions developed earlier for extracting keypoints, descriptors, and encoding images. Ensure that you use the visual vocabulary constructed with the training images for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_test_features = {\n",
    "    'SIFT': {},\n",
    "    'ORB': {}\n",
    "}\n",
    "\n",
    "for detector_name, detector in [('SIFT', sift), ('ORB', orb)]:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Encoding test images - {detector_name}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for percentage in data_percentages:\n",
    "        \n",
    "        vocabulary = vocabularies[detector_name][percentage]\n",
    "\n",
    "        # encode all test images \n",
    "        bovw, valid_indices = encode_dataset_bovw(\n",
    "            test_images,\n",
    "            test_labels,\n",
    "            detector,\n",
    "            vocabulary,\n",
    "            detector_name,\n",
    "            upscale_factor\n",
    "        )\n",
    "\n",
    "        encoded_test_features[detector_name][percentage] = {\n",
    "            'features': bovw,\n",
    "            'labels': test_labels[valid_indices],\n",
    "            'valid_indices': valid_indices\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-7\"></a>\n",
    "### **Section 7: Training the Classifiers (5 points)**\n",
    "\n",
    "In this section, the task is to create two one-vs-rest (OvR) SVM classifiers using the 50% of the training data that was **not** used for creating the visual dictionary. This ensures that the classifiers are trained on a different subset of data, providing a more robust evaluation of the visual vocabulary's effectiveness.\n",
    "\n",
    "For each of the two selected feature extraction techniques, create one-vs-rest classifiers for all classes. For now, use default parameter values when training the classifiers; you will experiment with different hyperparameters in later steps.\n",
    "\n",
    "**Note:** Training an OvR classifier can take around 5 to 7 minutes. Therefore, it's advisable to first test your code with a smaller subset of the training data to verify that your implementation works correctly before running it on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "# classifiers\n",
    "classifiers = {\n",
    "    'SIFT': OneVsRestClassifier(SVC(kernel='linear')),\n",
    "    'ORB': OneVsRestClassifier(SVC(kernel='linear'))\n",
    "}\n",
    "\n",
    "for detector_name, clf in classifiers.items():\n",
    "    print(f\"Training {detector_name} classifier...\")\n",
    "\n",
    "    train_x = encoded_train_features[detector_name][0.5]['features']\n",
    "    train_y = encoded_train_features[detector_name][0.5]['labels']\n",
    "\n",
    "    # we have to discard the first 50% of training data as we did use it for building the vocabulary\n",
    "    train_x = train_x[ (len(train_x) // 2) : ]\n",
    "    train_y = train_y[ (len(train_y) // 2) : ]\n",
    "\n",
    "    print(f\"Training data shape: {train_x.shape}, Training labels shape: {train_y.shape}\")\n",
    "\n",
    "    print(\"Fitting classifier...\")\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    print(f\"{detector_name} training complete!\")\n",
    "    \n",
    "\n",
    "    y_train_pred = clf.predict(train_x)\n",
    "    train_acc = (y_train_pred == np.array(train_y)).mean()\n",
    "    print(f\"Training accuracy: {train_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-8\"></a>\n",
    "### **Section 8: Evaluating the Classifiers (12 points)**\n",
    "\n",
    "In this section, you will evaluate the performance of your one-vs-rest (OvR) SVM classifiers on the test data. The goal is to classify each test image using each binary classifier and rank the images based on the classification scores, resulting in a ranked list of images for each class. Ideally, images belonging to the target class should appear at the top of the respective list. To conduct this evaluation, use the test image histograms generated earlier for the two selected feature extraction techniques. Classify each test image with each classifier, rank them based on their confidence scores, and then compute the Mean Average Precision (mAP) across all classes. The mAP for a single class $c$ is defined as:\n",
    "\n",
    "$\n",
    "\\text{mAP}_c = \\frac{1}{m_c} \\sum_{i=1}^{n} \\frac{f_c(x_i)}{i}\n",
    "$\n",
    "\n",
    "where:\n",
    "- $n$ is the total number of images ($n = 50 \\times 5 = 250$),\n",
    "- $m_c$ is the number of images of class $c$ ($m_c = 50$),\n",
    "- $x_i$ is the $i^{th}$ image in the ranked list $X = \\{ x_1, x_2, \\dots, x_n \\}$,\n",
    "- $f_c$ is a function that returns the number of images of class $c$ in the first $i$ images if $x_i$ is of class $c$, and 0 otherwise.\n",
    "\n",
    "For instance, if you are retrieving images of class \"R\" and the sequence of ranked images is $[R, R, T, R, T, T, R, T]$, then $n = 8$, $m_c = 4$, and:\n",
    "\n",
    "$\n",
    "AP = \\frac{1}{4} \\left( \\frac{1}{1} + \\frac{2}{2} + \\frac{0}{3} + \\frac{3}{4} + \\frac{0}{5} + \\frac{0}{6} + \\frac{4}{7} + \\frac{0}{8} \\right).\n",
    "$\n",
    "\n",
    "In addition to the quantitative analysis, perform a qualitative analysis by visualizing the top-5 and bottom-5 ranked test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_AP(ground_truth, scores, target_class):\n",
    "    \"\"\"\n",
    "    Compute Average Precision for a single class\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: true labels \n",
    "        scores: confidence scores for target_class\n",
    "        target_class: index of the class to compute AP for\n",
    "    \n",
    "    Returns:\n",
    "        ap: Average Precision for this class\n",
    "    \"\"\"\n",
    "    # sort images by confidence     \n",
    "    sorted_indices = np.argsort(scores)  # get sorted indices\n",
    "    sorted_indices = sorted_indices[::-1].copy()  # descending order\n",
    "    sorted_labels = ground_truth[sorted_indices]  # sort labels by indices\n",
    "\n",
    "    # number of images of target class\n",
    "    m_c = np.sum(ground_truth == target_class)\n",
    "    if m_c == 0: return 0.0\n",
    "    \n",
    "\n",
    "    ap = 0.0\n",
    "    num_correct = 0\n",
    "    \n",
    "    for i, label in enumerate(sorted_labels, start=1):\n",
    "        if label == target_class:\n",
    "            num_correct += 1\n",
    "            ap += num_correct / i\n",
    "    \n",
    "    ap *= 1 / m_c\n",
    "    \n",
    "    return ap\n",
    "\n",
    "\n",
    "\n",
    "def calc_mAP(ground_truth, scores, num_classes):\n",
    "    \"\"\"\n",
    "    Compute mean Average Precision over all classes\n",
    "    \n",
    "    Args:\n",
    "        ground_truth: true labels\n",
    "        scores: confidence scores for each class (num_images, num_classes)\n",
    "        num_classes: number of classes\n",
    "    \n",
    "    Returns:\n",
    "        mAP: mean Average Precision\n",
    "        APs: list of AP for each class\n",
    "    \"\"\"\n",
    "    APs = []\n",
    "    \n",
    "    for class_idx in range(num_classes):\n",
    "        ap = calc_AP(ground_truth, scores[:, class_idx], class_idx)\n",
    "        APs.append(ap)\n",
    "    \n",
    "    mAP = np.mean(APs)\n",
    "    \n",
    "    return mAP, APs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_top_bottom_ranked_images(n, decision_scores, ground_truth, detector_name):\n",
    "    \"\"\"\n",
    "    Visualize top-n and bottom-n ranked images for each class\n",
    "    Args:\n",
    "        n: number of top and bottom images to show\n",
    "        ground_truth: true labels\n",
    "        decision_scores: confidence scores for each class\n",
    "        detector_name: name of the detector\n",
    "    \"\"\"\n",
    "    \n",
    "    # get test images\n",
    "    valid_test_images = test_images[encoded_test_features[detector_name][0.5]['valid_indices']]\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "\n",
    "        # sort images by confidence\n",
    "        class_scores = decision_scores[:, class_idx]  # get scores for this class\n",
    "        sorted_indices = np.argsort(class_scores)  # get sorted indices\n",
    "        sorted_indices = sorted_indices[::-1].copy()  # descending order\n",
    "\n",
    "        fig, axes = plt.subplots(2, n, figsize=(15, 6))\n",
    "\n",
    "        fig.suptitle(f'{class_names[class_idx]} - {detector_name}\\nTop {n} and bottom {n} (right) ranked images', fontweight='bold')\n",
    "\n",
    "        # top-n images \n",
    "        for i in range(n):\n",
    "            index = sorted_indices[i]\n",
    "\n",
    "            img = denormalize_image(valid_test_images[index])\n",
    "            true_label = ground_truth[index]\n",
    "\n",
    "            axes[0, i].imshow(img)\n",
    "            axes[0, i].axis('off')\n",
    "            \n",
    "        \n",
    "            color = 'green' if true_label == class_idx else 'red'\n",
    "            axes[0, i].set_title(f'Rank {i+1}\\n{class_names[true_label]}', color=color, fontweight='bold')\n",
    "\n",
    "        # last n images \n",
    "        for i in range(n):\n",
    "            index = sorted_indices[-(n-i)]\n",
    "            img = denormalize_image(valid_test_images[index])\n",
    "            true_label = ground_truth[index]\n",
    "            \n",
    "            axes[1, i].imshow(img)\n",
    "            axes[1, i].axis('off')\n",
    "            \n",
    "            color = 'green' if true_label == class_idx else 'red'\n",
    "            axes[1, i].set_title(f'Rank {len(sorted_indices)-(n-i-1)}\\n{class_names[true_label]}', color=color, fontweight='bold')\n",
    "\n",
    "\n",
    "        axes[0, 0].text(-0.3, 0.5, f\"top {n}\", transform=axes[0, 0].transAxes, verticalalignment='center')\n",
    "        axes[1, 0].text(-0.3, 0.5, f\"bottom {n}\", transform=axes[1, 0].transAxes, verticalalignment='center')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for detector_name in ['SIFT', 'ORB']:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating {detector_name} Classifier\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Get test data\n",
    "    test_x =  np.array(encoded_test_features[detector_name][0.5]['features'])\n",
    "    test_y = np.array(encoded_test_features[detector_name][0.5]['labels'])\n",
    "    \n",
    "    # Get trained classifier\n",
    "    clf = classifiers[detector_name]\n",
    "    \n",
    "    # Get decision scores for ranking\n",
    "    decision_scores = clf.decision_function(test_x)\n",
    "    \n",
    "    print(f\"test set: {test_x.shape[0]} images\")\n",
    "    print(f\"decision scores shape: {decision_scores.shape}\")\n",
    "    \n",
    "    # Compute mAP\n",
    "    mAP, APs = calc_mAP(test_y, decision_scores, num_classes=num_classes)\n",
    "    results[detector_name] = {\n",
    "        \"mAP\": mAP,\n",
    "        \"APs\": APs\n",
    "    }\n",
    "\n",
    "    print(f\"\\n{detector_name} mAP: {mAP*100:.2f}%\")\n",
    "    for class_idx, ap in enumerate(APs):\n",
    "        print(f\"Class {class_idx} AP: {ap*100:.2f}%\")\n",
    "\n",
    "\n",
    "    visualize_top_bottom_ranked_images(5, decision_scores, test_y, detector_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-9\"></a>\n",
    "### **Section 9: Hyperparameter Search (16 points)**\n",
    "\n",
    "In this section, the task is to perform an extensive hyperparameter search to optimize the performance of your classifiers. You will experiment with various parameters, including the number of visual words (e.g., 500, 1000, 1500), different training subset sizes (e.g., 30%, 40%, 50%), SVM parameters (e.g., kernel types like 'linear' or 'rbf', regularization parameter $C$ values such as 0.1, 1, 10, and gamma settings like 'scale' or specific values such as 0.01, 0.001), and settings of the feature extractors (e.g., the number of keypoints or scale levels). Start by testing your code on the smallest subset to ensure it functions correctly before proceeding with a full hyperparameter search. Once validated, conduct the search using larger subsets and systematically iterate through the different parameter combinations, potentially using nested loops or grid search. Be sure to record the performance results for each combination to identify the best settings based on metrics like the Mean Average Precision (mAP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor parameters\n",
    "upscale_factor_options = [2, 4, 8]\n",
    "detector_types = ['SIFT', 'ORB']\n",
    "nfeatures_options = [500, 1000]\n",
    "\n",
    "# visual vocabulary parameters\n",
    "num_visual_words_options = [500, 1000, 1500]\n",
    "training_subset_sizes = [0.3, 0.4, 0.5]\n",
    "\n",
    "\n",
    "# support vector machine parameters\n",
    "svm_kernels = ['linear', 'rbf']\n",
    "svm_C_values = [0.1, 1, 10]\n",
    "svm_gamma_values = ['scale', 0.01, 0.001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitAndEvaluationParams:\n",
    "    def __init__(self, detector_type, nfeatures, upscale_factor, num_visual_words, training_subset_size, svm_kernel, svm_C, svm_gamma):\n",
    "        self.detector_type = detector_type\n",
    "        self.nfeatures = nfeatures\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.num_visual_words = num_visual_words\n",
    "        self.training_subset_size = training_subset_size\n",
    "        self.svm_kernel = svm_kernel\n",
    "        self.svm_C = svm_C\n",
    "        self.svm_gamma = svm_gamma\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"Detector: {self.detector_type}, nfeatures: {self.nfeatures}, \"\n",
    "                f\"Upscale: {self.upscale_factor}, Visual Words: {self.num_visual_words}, \"\n",
    "                f\"Subset Size: {self.training_subset_size}, SVM Kernel: {self.svm_kernel}, \"\n",
    "                f\"C: {self.svm_C}, Gamma: {self.svm_gamma}\")\n",
    "\n",
    "\n",
    "class FitAndEvaluationTestResult:\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.mAP = None\n",
    "        self.test_acc = None\n",
    "        self.training_time = None\n",
    "        self.vocabulary_time = None\n",
    "\n",
    "\n",
    "    def set_results(self, mAP, test_acc, training_time, vocabulary_time):\n",
    "        self.mAP = mAP\n",
    "        self.test_acc = test_acc\n",
    "        self.training_time = training_time\n",
    "        self.vocabulary_time = vocabulary_time\n",
    "    \n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def perform_fit_and_evaluation_test(params):\n",
    "    \"\"\"\n",
    "    Fit and evaluate BoVW + SVM classifier with given parameters\n",
    "\n",
    "    args:\n",
    "        params: FitAndEvaluationParams \n",
    "    \"\"\"\n",
    "\n",
    "    upscale_factor, detector_type, nfeatures, num_visual_words, vocabulary_percentage, svm_kernel, svm_C, svm_gamma = (\n",
    "        params.upscale_factor,\n",
    "        params.detector_type,\n",
    "        params.nfeatures,\n",
    "        params.num_visual_words,\n",
    "        params.training_subset_size,\n",
    "        params.svm_kernel,\n",
    "        params.svm_C,\n",
    "        params.svm_gamma\n",
    "    )\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Testing configuration:\")\n",
    "    print(params)\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1) Initialize detector \n",
    "    if detector_type == 'SIFT': detector = cv2.SIFT_create(nfeatures=nfeatures)\n",
    "    elif detector_type == 'ORB': detector = cv2.ORB_create(nfeatures=nfeatures)\n",
    "    else: raise ValueError(\"detector_type must be 'SIFT' or 'ORB'\")\n",
    "\n",
    "    # 2) Upscale images and extract descriptors from training data for vocabulary creation\n",
    "    start_vocab_time = time.time()\n",
    "    voc_descriptors, voc_num_images = extract_all_descriptors(\n",
    "        train_images, detector, vocabulary_percentage, upscale_factor\n",
    "    )\n",
    "\n",
    "    if len(voc_descriptors) == 0: raise ValueError(\"No descriptors extracted from training data\")\n",
    "\n",
    "    voc_descriptors = voc_descriptors.astype(np.float32)\n",
    "\n",
    "    # 3) Build visual vocabulary\n",
    "    k_means, visual_words = build_visual_vocabulary(\n",
    "        voc_descriptors, \n",
    "        n_clusters=num_visual_words\n",
    "    )\n",
    "\n",
    "    vocabulary = {\n",
    "        'kmeans': k_means,\n",
    "        'cluster_centers': visual_words,\n",
    "        'num_descriptors': len(voc_descriptors),\n",
    "        'num_images': voc_num_images\n",
    "    }\n",
    "\n",
    "    end_vocab_time = time.time()\n",
    "    vocabulary_time = end_vocab_time - start_vocab_time\n",
    "\n",
    "    print(f\"\\nVocabulary built with {len(voc_descriptors):,} descriptors from {voc_num_images} images. Creation time: {vocabulary_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "    # 4) Encode training images\n",
    "    # We have to consider only the second part of training data as the first percentage% was used for building the vocabulary\n",
    "    # skip the images used for vocabulary creation (percentage% of training data)\n",
    "    start_index = int(len(train_images) * vocabulary_percentage)\n",
    "    remaining_train_images = train_images[start_index:]\n",
    "    remaining_train_labels = train_labels[start_index:]\n",
    "\n",
    "\n",
    "    train_x, train_valid_indices = encode_dataset_bovw(\n",
    "        remaining_train_images,\n",
    "        remaining_train_labels,\n",
    "        detector,\n",
    "        vocabulary,\n",
    "        detector_type,\n",
    "        upscale_factor\n",
    "    )\n",
    "\n",
    "    train_y = remaining_train_labels[train_valid_indices]\n",
    "\n",
    "    # 5) Train SVM classifier\n",
    "    print(\"Fitting classifier...\")\n",
    "    print(f\"Training data shape: {train_x.shape}, Training labels shape: {train_y.shape}\")\n",
    "\n",
    "\n",
    "    if svm_kernel == 'linear': clf = OneVsRestClassifier(SVC(kernel=svm_kernel, C=svm_C, probability=False))\n",
    "    else: clf = OneVsRestClassifier(SVC(kernel=svm_kernel, C=svm_C, gamma=svm_gamma, probability=False))\n",
    "\n",
    "\n",
    "    startTime = time.time()\n",
    "\n",
    "    clf.fit(train_x, train_y)\n",
    "\n",
    "    endTime = time.time()\n",
    "    training_time = endTime - startTime\n",
    "    print(f\"Training complete in {training_time:.2f} seconds\")\n",
    "\n",
    "    # 6) Accuracy evaluation on test data\n",
    "    test_x, test_valid_indices = encode_dataset_bovw(\n",
    "        test_images,\n",
    "        test_labels,\n",
    "        detector,\n",
    "        vocabulary,\n",
    "        detector_type,\n",
    "        upscale_factor\n",
    "    )\n",
    "\n",
    "    test_y = test_labels[test_valid_indices]\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    y_test_pred = clf.predict(test_x)\n",
    "    test_acc = (y_test_pred == np.array(test_y)).mean()\n",
    "    print(f\"Test accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "    # 7) mAP evaluation on test data\n",
    "    decision_scores = clf.decision_function(test_x)\n",
    "    mAP, _ = calc_mAP(test_y, decision_scores, num_classes=num_classes)\n",
    "\n",
    "    print(f\"{detector_type} mAP: {mAP*100:.2f}%\")\n",
    "\n",
    "    # Save results\n",
    "    result = FitAndEvaluationTestResult(params)\n",
    "    result.set_results(\n",
    "        mAP=mAP,\n",
    "        test_acc=test_acc,\n",
    "        training_time=training_time,\n",
    "        vocabulary_time=vocabulary_time\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "test_params = FitAndEvaluationParams(\n",
    "    detector_type='SIFT',\n",
    "    nfeatures=1000,\n",
    "    upscale_factor=4,\n",
    "    num_visual_words=1000,\n",
    "    training_subset_size=0.2,\n",
    "    svm_kernel='linear',\n",
    "    svm_C=1,\n",
    "    svm_gamma='scale'\n",
    ")\n",
    "\n",
    "test_result = perform_fit_and_evaluation_test(test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd\n",
    "\n",
    "def build_dataframe_from_results(results):\n",
    "    \"\"\"\n",
    "    Build pandas DataFrame from results list\n",
    "    \"\"\"\n",
    "    results_data = []\n",
    "    for r in results:\n",
    "        results_data.append({\n",
    "            'detector': r.params.detector_type,\n",
    "            'nfeatures': r.params.nfeatures,\n",
    "            'upscale_factor': r.params.upscale_factor,\n",
    "            'num_visual_words': r.params.num_visual_words,\n",
    "            'training_subset_size': r.params.training_subset_size,\n",
    "            'svm_kernel': r.params.svm_kernel,\n",
    "            'svm_C': r.params.svm_C,\n",
    "            'svm_gamma': r.params.svm_gamma,\n",
    "            'mAP': r.mAP,\n",
    "           # 'test_accuracy': r.test_acc,\n",
    "            'training_time': r.training_time,\n",
    "            'vocabulary_time': r.vocabulary_time,\n",
    "            'total_time': r.training_time + r.vocabulary_time\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(results_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "def save_results(results):\n",
    "    \"\"\"\n",
    "    Save hyperparameter search results to CSV \n",
    "    \"\"\"\n",
    "  \n",
    "    filename = f\"hyperparam_results\"\n",
    "    results = [r for r in results if r.mAP is not None]\n",
    "    df = build_dataframe_from_results(results)\n",
    "    csv_filename = f\"{filename}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    print(f\"Results saved to {csv_filename}\")\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def plot_results(df, save_path=None):\n",
    "    \"\"\"\n",
    "    Visualization of hyperparameter search results\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe with results\n",
    "        save_path: Path to save the figure (default: don't save)\n",
    "    \"\"\"\n",
    "  \n",
    "    # Create figure with subplots\n",
    "    fig = plt.figure(figsize=(20, 9))\n",
    "    \n",
    "    # 1. mAP by detector type\n",
    "    ax1 = plt.subplot(3, 4, 1)\n",
    "    df.boxplot(column='mAP', by='detector', ax=ax1)\n",
    "    ax1.set_title('mAP by Detector Type')\n",
    "    ax1.set_xlabel('Detector')\n",
    "    ax1.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 2. mAP by number of features\n",
    "    ax2 = plt.subplot(3, 4, 2)\n",
    "    df.boxplot(column='mAP', by='nfeatures', ax=ax2)\n",
    "    ax2.set_title('mAP by Number of Features')\n",
    "    ax2.set_xlabel('Number of Features')\n",
    "    ax2.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 3. mAP by upscale factor\n",
    "    ax3 = plt.subplot(3, 4, 3)\n",
    "    df.boxplot(column='mAP', by='upscale_factor', ax=ax3)\n",
    "    ax3.set_title('mAP by Upscale Factor')\n",
    "    ax3.set_xlabel('Upscale Factor')\n",
    "    ax3.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 4. mAP by visual words\n",
    "    ax4 = plt.subplot(3, 4, 4)\n",
    "    df.boxplot(column='mAP', by='num_visual_words', ax=ax4)\n",
    "    ax4.set_title('mAP by Visual Words')\n",
    "    ax4.set_xlabel('Number of Visual Words')\n",
    "    ax4.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 5. mAP by training subset size\n",
    "    ax5 = plt.subplot(3, 4, 5)\n",
    "    df.boxplot(column='mAP', by='training_subset_size', ax=ax5)\n",
    "    ax5.set_title('mAP by Training Subset Size')\n",
    "    ax5.set_xlabel('Training Subset Size')\n",
    "    ax5.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 6. mAP by SVM kernel\n",
    "    ax6 = plt.subplot(3, 4, 6)\n",
    "    df.boxplot(column='mAP', by='svm_kernel', ax=ax6)\n",
    "    ax6.set_title('mAP by SVM Kernel')\n",
    "    ax6.set_xlabel('SVM Kernel')\n",
    "    ax6.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 7. mAP by SVM C\n",
    "    ax7 = plt.subplot(3, 4, 7)\n",
    "    df.boxplot(column='mAP', by='svm_C', ax=ax7)\n",
    "    ax7.set_title('mAP by SVM C')\n",
    "    ax7.set_xlabel('SVM C')\n",
    "    ax7.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 8. mAP by SVM gamma\n",
    "    ax8 = plt.subplot(3, 4, 8)\n",
    "    df.boxplot(column='mAP', by='svm_gamma', ax=ax8)\n",
    "    ax8.set_title('mAP by SVM Gamma')\n",
    "    ax8.set_xlabel('SVM Gamma')\n",
    "    ax8.set_ylabel('mAP')\n",
    "    plt.suptitle('')\n",
    "    \n",
    "    # 9. Top 10 configurations\n",
    "    ax9 = plt.subplot(3, 4, 9)\n",
    "    top10 = df.nlargest(10, 'mAP').reset_index(drop=True)\n",
    "    bars = ax9.barh(range(len(top10)), top10['mAP'])\n",
    "    ax9.set_yticks(range(len(top10)))\n",
    "    ax9.set_yticklabels([f\"Config {i+1}\" for i in range(len(top10))])\n",
    "    ax9.set_xlabel('mAP')\n",
    "    ax9.set_title('Top 10 Configurations by mAP')\n",
    "    ax9.invert_yaxis()\n",
    "    for i, bar in enumerate(bars):\n",
    "        ax9.text(bar.get_width(), bar.get_y() + bar.get_height()/2, \n",
    "                f'{top10.iloc[i][\"mAP\"]:.3f}', \n",
    "                va='center', ha='left', fontsize=8)\n",
    "    \n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path: plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Print insights\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Best detector\n",
    "    best_detector = df.groupby('detector')['mAP'].mean().idxmax()\n",
    "    print(f\"Best detector on average: {best_detector} (mean mAP: {df.groupby('detector')['mAP'].mean().max():.3f})\")\n",
    "    \n",
    "    # Best kernel\n",
    "    best_kernel = df.groupby('svm_kernel')['mAP'].mean().idxmax()\n",
    "    print(f\"Best SVM kernel on average: {best_kernel} (mean mAP: {df.groupby('svm_kernel')['mAP'].mean().max():.3f})\")\n",
    "    \n",
    "    # Best configuration\n",
    "    best_config = df.loc[df['mAP'].idxmax()]\n",
    "    print(f\"\\nBest overall configuration (mAP: {best_config['mAP']:.3f}):\")\n",
    "    print(f\"  - Detector: {best_config['detector']}\")\n",
    "    print(f\"  - Features: {best_config['nfeatures']}\")\n",
    "    print(f\"  - Upscale: {best_config['upscale_factor']}\")\n",
    "    print(f\"  - Visual Words: {best_config['num_visual_words']}\")\n",
    "    print(f\"  - Training Subset: {best_config['training_subset_size']}\")\n",
    "    print(f\"  - Kernel: {best_config['svm_kernel']}\")\n",
    "    print(f\"  - C: {best_config['svm_C']}\")\n",
    "    print(f\"  - Gamma: {best_config['svm_gamma']}\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# After your grid search loop:\n",
    "# df = save_results_to_file(results)\n",
    "# plot_results(results, save_path='hyperparameter_search_plots.png')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter grid search\n",
    "import itertools\n",
    "\n",
    "param_combinations = list(itertools.product(\n",
    "    detector_types[0:2],\n",
    "    nfeatures_options[0:2],\n",
    "    upscale_factor_options[0:2],\n",
    "    num_visual_words_options[0:2],\n",
    "    training_subset_sizes[0:2],\n",
    "    svm_kernels[0:2],\n",
    "    svm_C_values[0:2],\n",
    "    svm_gamma_values[0:2]\n",
    "))\n",
    "\n",
    "print(f\"\\nTotal parameter combinations to test: {len(param_combinations)}\")\n",
    "\n",
    "results = []\n",
    "for param_tuple in param_combinations:\n",
    "    params = FitAndEvaluationParams(\n",
    "        detector_type=param_tuple[0],\n",
    "        nfeatures=param_tuple[1],\n",
    "        upscale_factor=param_tuple[2],\n",
    "        num_visual_words=param_tuple[3],\n",
    "        training_subset_size=param_tuple[4],\n",
    "        svm_kernel=param_tuple[5],\n",
    "        svm_C=param_tuple[6],\n",
    "        svm_gamma=param_tuple[7]\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        result = perform_fit_and_evaluation_test(params)\n",
    "        results.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error with parameters {params}: {e}\")\n",
    "\n",
    "\n",
    "# Save results to CSV\n",
    "save_results(results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(results)\n",
    "plot_results(build_dataframe_from_results(results), save_path='hyperparameter_search_plots.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-10\"></a>\n",
    "### **Section 10: Using CLIP for Image Classification (5 points)**\n",
    "\n",
    "**<span style=\"color:red\">⚠️ NOTE: This section should NOT be included in the report. It is only meant to be completed in the code cells. The purpose of this task is to introduce you to a more state-of-the-art model (CLIP) compared to Bag of Visual Words (BoVW). Vision Transformers (ViT) will be covered in more detail in the Deep Learning 1 course next period!</span>**\n",
    "\n",
    "In this section, you will use a pre-trained CLIP model for image classification. CLIP (Contrastive Language-Image Pretraining) is a vision-language transformer model trained on a large dataset of images and text. It consists of two main components: a Vision Transformer (ViT) and a text Transformer. The ViT encodes images by dividing them into patches (tokens), flattening each patch into a vector, and passing them through a sequence of Transformer layers to produce an encoded representation of the image.\n",
    "\n",
    "For this task, you will use the visual transformer component of CLIP to extract encoded representations of the input images. While this is not the typical way to use CLIP (which involves encoding both images and text for similarity comparison), it provides an interesting application of this state-of-the-art model for image classification.\n",
    "\n",
    "**To Install CLIP:**\n",
    "```python\n",
    "pip install git+https://github.com/openai/CLIP.git\n",
    "```\n",
    "\n",
    "**Additional Reading (if you're interested):**\n",
    "- [OpenAI CLIP Overview](https://openai.com/clip)\n",
    "- [Vision Transformer (ViT) Paper](https://arxiv.org/abs/2010.11929)\n",
    "- [Tutorial on Vision Transformers](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)\n",
    "- [UvA's Deep Learning Introduction to ViTs](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial15/Vision_Transformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create DataLoaders for both the training and test datasets by filtering the CIFAR-10 dataset to include only the selected classes: frog, automobile, bird, cat, and deer. Use a batch size of 16 for both DataLoaders, and resize the images to 224x224 to match the input size requirements for CLIP. Remember to normalize the images using the appropriate mean and standard deviation. Use a training set size of 1000 images per class and a test set size of 200 images per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load a pre-trained CLIP model, which is a vision-language transformer designed to predict the text that describes an image and vice versa. The model consists of two components: a Vision Transformer (ViT) for encoding images and a text Transformer for encoding text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "\n",
    "# Setup the model and the preprocessor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the pre-trained CLIP model\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the visual tokens using the CLIP model, start by initializing two empty lists: one for storing the image features and another for the labels. Use `tqdm` to create a progress bar that tracks the extraction process over the DataLoader. Iterate through the DataLoader, extracting images and labels. Disable gradient computation, and then encode the images with `model.encode_image(images)`. Append the encoded features and labels to their respective lists. Remember, the output from the model will have the shape `(batch_size, 512)` due to batched processing, if your are using a single you should reshape the output to `(512,)` to remove the batch dimension.\n",
    "\n",
    "In this example, we will use the class token as the visual representation of the image. The class token is a 512-dimensional vector that represents the image. We will use this vector to train a classifier to classify the images.\n",
    "\n",
    "**Note:** The CLIP model is quite large and may take some time to extract features from the images. You can use the `tqdm` library to create a progress bar that shows the extraction progress. It is recommended to test the code with a smaller subset of images to ensure it functions correctly before running it on the full dataset. With the default batch size of 16 and a training set size of 1000 images, the extraction process may take a 10-20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack the (batched) visual tokens extracted from the images. The resulting shape will be (number_of_images, 512).\n",
    "stacked_image_features = torch.cat(image_features_list)\n",
    "stacked_target_labels = torch.cat(target_labels_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a classifier using the visual tokens, start by initializing an SVM classifier using `SVC()` from `scikit-learn`. You can play around with different hyperparameters such as kernel type, regularization parameter, and gamma to find the best configuration. Finally, use the `fit` method to train the classifier on the visual image features and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the classifier, start by extracting the visual tokens from the test images using the same method applied to the training set. Loop through the test DataLoader, encode each batch of images using the model (e.g., `model.encode_image(images)`), and store the results in separate lists for the features and labels. After extracting all the features, stack them into a single tensor for both the features and labels. This process will prepare the test data for use in evaluating the classifier's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of your classifier, use the test set features you extracted earlier. First, generate predictions for the test set by passing the stacked test features into your trained classifier's `predict` method. Next, use the `classification_report` function from `sklearn.metrics` to create a detailed report that includes metrics such as precision, recall, and F1-score for each class. Finally, print the report to analyze how well your classifier performs across the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-x\"></a>\n",
    "### **Section X: Individual Contribution Report *(Mandatory)***\n",
    "\n",
    "Because we want each student to contribute fairly to the submitted work, we ask you to fill out the textcells below. Write down your contribution to each of the assignment components in percentages. Naturally, percentages for one particular component should add up to 100% (e.g. 30% - 30% - 40%). No further explanation has to be given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | Contribution on Research | Contribution on Programming | Contribution on Writing |\n",
    "| -------- | ------- | ------- | ------- |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |\n",
    "|  | - % | - % | - % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - End of Notebook -"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
